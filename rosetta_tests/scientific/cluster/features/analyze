#!/bin/bash
#
# This is a command file.
#
# The contents of this file will be used to create a Bash shell script
# so any legal shell commands can go in this file.
# Or comments like this one, for that matter.
#
# Variable substiution is done using Python's printf format,
# meaning you need a percent sign, the variable name in parentheses,
# and the letter 's' (for 'string').
#
# Available variables include:
#   workdir     the directory where test input files have been copied,
#               and where test output files should end up.
#   minidir     the base directory where Mini lives
#   database    where the Mini database lives
#   bin         where the Mini binaries live
#   binext      the extension on binary files, like ".linuxgccrelease"
#
# The most important thing is that the test execute in the right directory.
# This is especially true when we're using SSH to execute on other hosts.
# All command files should start with this line:
#
# This command files is used for two cluster scripts: submit and analyze
# 'submit' script should prepare all input files and submit them to a condor.
#          After execution of 'submit' is finished root level script will wait until condor queue is
#          empty and create and call 'analyze' script.
#
# 'analyze' script should assume that all condor jobs are finished and can perform all post process
#           calculations thats are necessary. The end result of analyze script is at least two
#           files: results.log and results.yaml. In addition to that all files that was left in
#           output/ directory will be also saved by root script for future references.


# prepare feature databases for caching into the testing server

echo storing feature databased to the files folder

sample_sources_fname="%(workdir)s/sample_sources/benchmark.list"
while read sample_source; do
	echo $sample_source | grep -q -e '^#' -e '^$' && continue

	echo Begin processing sample source, $sample_source
	for features_db_output in `find sample_sources/$sample_source/output/ -type d -name $sample_source*`; do
		echo '	Begin processing features database' $features_db_output
		sample_source_id="$(basename $features_db_output)"

		echo '    Merge database parts together in case the features database was generated on a cluster'
		features_db=$features_db_output/features_${sample_source_id}.db3
		features_db_parts=$(ls ${features_db}_*)
		python sample_sources/merge_databases.py $features_db $features_db_parts

		echo '    Setup output paths'
		target_dir=files/$sample_source_id
		compressed_fname=$target_dir/$(basename $features_db).tar.gz
		mkdir -p $target_dir

		#find $features_db_output -name \*.log -exec cp {} /logs \;

		echo '    Compressing features database -> ' $compressed_fname
		tar -czf $compressed_fname $features_db

		echo '    Splitting compressed features database -> ' ${compressed_fname}_???
		rm -f ${compressed_fname}_*
		split -a 3 -b 10m $compressed_fname ${compressed_fname}_
		rm $compressed_fname
	done
	echo ""
done < $sample_sources_fname
