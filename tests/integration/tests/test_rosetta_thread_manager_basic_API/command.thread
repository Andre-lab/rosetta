#
# This is a command file.
#
# To make a new test, all you have to do is:
#   1.  Make a new directory under tests/
#   2.  Put a file like this (named "command") into that directory.
#
# The contents of this file will be passed to the shell (Bash or SSH),
# so any legal shell commands can go in this file.
# Or comments like this one, for that matter.
#
# Variable substiution is done using Python's printf format,
# meaning you need a percent sign, the variable name in parentheses,
# and the letter 's' (for 'string').
#
# Available variables include:
#   workdir     the directory where test input files have been copied,
#               and where test output files should end up.
#   minidir     the base directory where Mini lives
#   database    where the Mini database lives
#   bin         where the Mini binaries live
#   binext      the extension on binary files, like ".linuxgccrelease"
#
# The most important thing is that the test execute in the right directory.
# This is especially true when we're using SSH to execute on other hosts.
# All command files should start with this line:
#

cd %(workdir)s

[ -x %(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s ] || exit 1
%(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s %(additional_flags)s @flags_nothreads -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
    | egrep -vf ../../ignore_list \
    > log_nothreads

test "${PIPESTATUS[0]}" != '0' && exit 1 || true  # Check if the first executable in pipe line return error and exit with error code if so

[ -x %(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s ] || exit 1
%(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s %(additional_flags)s @flags1 -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
    | egrep -vf ../../ignore_list \
    > log1

test "${PIPESTATUS[0]}" != '0' && exit 1 || true  # Check if the first executable in pipe line return error and exit with error code if so

[ -x %(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s ] || exit 1
%(bin)s/test_rosetta_thread_manager_basic_API.%(binext)s %(additional_flags)s @flags2 -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
    | egrep -vf ../../ignore_list \
    > log2

test "${PIPESTATUS[0]}" != '0' && exit 1 || true  # Check if the first executable in pipe line return error and exit with error code if so


# Check that the version that generates each cell in a different thread produces 100 lines of unique output:
grep test_rosetta_thread_manager_basic_API_output log1 | sed 's/ 10/ 0/g' | sort -nk 3,102 > temp1.txt
grep test_rosetta_thread_manager_basic_API_output log1 | sed 's/ 10/ 0/g' | sort -nk 3,102 | uniq > temp1b.txt
test `diff temp1.txt temp1b.txt | wc -l` -eq '0' || exit 1

# Check that each thread had a unique random seed:
test `grep "basic.random.init_random_generator" log1 | grep "RandomGenerator:init" | awk '{print $6}' | sed 's/=/ /g' | awk '{print $2}' | sort -n | wc -l` -eq '20' || exit 1
test `grep "basic.random.init_random_generator" log1 | grep "RandomGenerator:init" | awk '{print $6}' | sed 's/=/ /g' | awk '{print $2}' | sort -n | uniq | wc -l` -eq '20' || exit 1

rm log1 temp1.txt temp1b.txt

# Check that the version that generates each row in a different thread produces 100 lines of unique output.
grep test_rosetta_thread_manager_basic_API_output log2 | sed 's/ 10/ 0/g' | sort -nk 3,102 > temp2.txt
grep test_rosetta_thread_manager_basic_API_output log2 | sed 's/ 10/ 0/g' | sort -nk 3,102 | uniq > temp2b.txt
test `diff temp2.txt temp2b.txt | wc -l` -eq '0' || exit 1

# Check that each thread had a unique random seed:
test `grep "basic.random.init_random_generator" log2 | grep "RandomGenerator:init" | awk '{print $6}' | sed 's/=/ /g' | awk '{print $2}' | sort -n | wc -l` -eq '20' || exit 1
test `grep "basic.random.init_random_generator" log2 | grep "RandomGenerator:init" | awk '{print $6}' | sed 's/=/ /g' | awk '{print $2}' | sort -n | uniq | wc -l` -eq '20' || exit 1

rm log2 temp2.txt temp2b.txt

# After that, do whatever you want.
# Files will be diffed verbatim, so if you want to log output and compare it,
# you'll need to filter out lines that change randomly (e.g. timings).
# Prefixing your tests with "nice" is probably good form as well.
# Don't forget to use -testing:INTEGRATION_TEST  so results are reproducible.
# Here's a typical test for a Mini binary, assuming there's a "flags" file
# in this directory too:
#
## %(bin)s/MY_MINI_PROGRAM.%(binext)s %(additional_flags)s @flags -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
##     | egrep -v 'Finished.+in [0-9]+ seconds.' \
##     | egrep -v 'Dunbrack library took .+ seconds to load' \
##     > log
#
# Or if you don't care whether the logging output changes:
#
## %(bin)s/MY_MINI_PROGRAM.%(binext)s %(additional_flags)s @flags -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
##     > /dev/null
#
