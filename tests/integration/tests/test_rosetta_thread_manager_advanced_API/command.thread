#
# This is a command file.
#
# To make a new test, all you have to do is:
#   1.  Make a new directory under tests/
#   2.  Put a file like this (named "command") into that directory.
#
# The contents of this file will be passed to the shell (Bash or SSH),
# so any legal shell commands can go in this file.
# Or comments like this one, for that matter.
#
# Variable substiution is done using Python's printf format,
# meaning you need a percent sign, the variable name in parentheses,
# and the letter 's' (for 'string').
#
# Available variables include:
#   workdir     the directory where test input files have been copied,
#               and where test output files should end up.
#   minidir     the base directory where Mini lives
#   database    where the Mini database lives
#   bin         where the Mini binaries live
#   binext      the extension on binary files, like ".linuxgccrelease"
#
# The most important thing is that the test execute in the right directory.
# This is especially true when we're using SSH to execute on other hosts.
# All command files should start with this line:
#

cd %(workdir)s

[ -x %(bin)s/test_rosetta_thread_manager_advanced_API.%(binext)s ] || exit 1
%(bin)s/test_rosetta_thread_manager_advanced_API.%(binext)s %(additional_flags)s @flags_nothreads -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
    | egrep -vf ../../ignore_list \
    > log_nothreads

test "${PIPESTATUS[0]}" != '0' && exit 1 || true  # Check if the first executable in pipe line return error and exit with error code if so

[ -x %(bin)s/test_rosetta_thread_manager_advanced_API.%(binext)s ] || exit 1
%(bin)s/test_rosetta_thread_manager_advanced_API.%(binext)s %(additional_flags)s @flags1 -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
    | egrep -vf ../../ignore_list \
    > log1

test "${PIPESTATUS[0]}" != '0' && exit 1 || true  # Check if the first executable in pipe line return error and exit with error code if so

# Check that we actually launched the correct number of threads:
test `grep "Level one thread" log1 | grep "reporting in" | wc -l` -eq '5' || exit 1
test `grep "Level two thread" log1 | grep "reporting in" | wc -l` -eq '15' || exit 1
test `grep "Level three thread" log1 | grep "reporting in" | wc -l` -eq '20' || exit 1

# Check that every thread computed at least one line in the times table:
test `grep "computing multiples of" log1 | awk '{print $2,$3,$4,$5,$6,$7,$8}' | sort | uniq | wc -l` -eq '20' || exit 1

# Check that we computed all of the entries in the times table:
test `grep "computing multiples of" log1 | wc -l` -eq '1000' || exit 1

# Check that we computed the times table properly:
grep "Results:" -A 1001 log1 >> log1_edited
rm log1

#
# After that, do whatever you want.
# Files will be diffed verbatim, so if you want to log output and compare it,
# you'll need to filter out lines that change randomly (e.g. timings).
# Prefixing your tests with "nice" is probably good form as well.
# Don't forget to use -testing:INTEGRATION_TEST  so results are reproducible.
# Here's a typical test for a Mini binary, assuming there's a "flags" file
# in this directory too:
#
## %(bin)s/MY_MINI_PROGRAM.%(binext)s %(additional_flags)s @flags -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
##     | egrep -v 'Finished.+in [0-9]+ seconds.' \
##     | egrep -v 'Dunbrack library took .+ seconds to load' \
##     > log
#
# Or if you don't care whether the logging output changes:
#
## %(bin)s/MY_MINI_PROGRAM.%(binext)s %(additional_flags)s @flags -database %(database)s -testing:INTEGRATION_TEST  2>&1 \
##     > /dev/null
#
