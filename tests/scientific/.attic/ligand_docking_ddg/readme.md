## AUTHOR AND DATE
#### Who set up the benchmark? Please add name, email, PI, month and year

Adapted for the current benchmarking framework by Shannon Smith (shannon.t.smith.1@vanderbilt.edu; Meiler Lab), Feb 2019 

## PURPOSE OF THE TEST
#### What does the benchmark test and why?

This benchmark is meant to test how well we discriminate native docking poses from decoys based on the interface score term by performing standard ligand docking experiments across 50 diverse protein-ligand complexes. 
This benchmark is meant to examine Rosetta Ligand's ability to rank similar compounds into the same active site. 

## BENCHMARK DATASET
#### How many proteins are in the set?
#### What dataset are you using? Is it published? If yes, please add a citation.
#### What are the input files? How were the they created?

A total of X datasets for binding affinity tests were taken from The Binding Database validation sets for computational chemistry (http://bindingdb.org/validation_sets/index.jsp). Protein-ligand complexes were then filtered by compound affinity (<1Î¼M) to be consistent with thresholds used in high-throughput screening assays with affinities spanning at least 3 orders of magnitude within a given test set. Ligands within a given set were highly similar and belonged to one compound series allowing us to assume similar binding poses. Additionally, all affinity values compounds within a set were obtained from the same group using the same experimental setup. Compounds were aligned to the co-crystallized ligand using the BCL alignment tool then subsequently run through high-resolution docking and a final minimization step to obtain a predicted score, which was then used to measure correlation to experimentally-determined affinities.

## PROTOCOL
#### State and briefly describe the protocol.
#### Is there a publication that describes the protocol?
#### How many CPU hours does this benchmark take approximately?

Structure preparation:
Proteins were extracted directly from the PDB according to their PDBIDs, underwent minimal cleaning to remove the ligand 

Ligand preparation:
Initial ligand structures were downloaded from PubChem. Ligand files were cleaned using OpenBabel (J. Cheminf. 2011, 3, 33), run through BCL Conformer Generator and generated Rosetta-readable parameter files according to the following scripts:
BCL Conformer Generator
bcl.exe molecule:ConformerGenerator -rotamer_library cod -top_models 100 -ensemble_filenames NAME.sdf -conformers_single_file NAME_conformers.sdf -conformation_comparer 'Dihedral(method=Max)' 30 -max_iterations 1000 

MolFileToParams Script:
/programs/x86_64-linux/rosetta/3.8/main/source/scripts/python/public/molfile_to_params.py -n $NAME -p $NAME --mm-as-virt --conformers-in-one-file NAME_conformers.sdf --chain X

Each dataset in this benchmark contains a cocrystal structure of the protein and one of the compounds in the set. The other, non-cocrystallized compounds were aligned to the cocrystallized pose using the BCL alignment protocol according to the following commmand, which was then used as the initial input.

This protocol currently uses 12 test cases, each containing the cleaned PDB protein file (without the ligand), the aligned ligand PDB file, ligand params file and ligand conformer library file. 

NOTE: the protein and ligand preparation steps were performed previously and are given as the input in the data/ directory. In other words, the preparation steps are not performed each time this benchmark is run.

ADJUST ME** Each test takes ~8 CPU hours x 50 tests = ~400 CPU hours.

## PERFORMANCE METRICS
#### What are the performance metrics used and why were they chosen?
#### How do you define a pass/fail for this test?
#### How were any cutoffs defined?
**Need to run several tests to see what we think should be the cutoff
How well do we discriminate native versus non-native binding poses? Generally with nstruct=1000, I've considered a success to be finding a sub-2A structure within the top 1% of models based on the interface_delta_X score. Should probably add in cross-docking cases, as well. 

## KEY RESULTS
#### What is the baseline to compare things to - experimental data or a previous Rosetta protocol?
#### Describe outliers in the dataset. 

The ranked compounds are compared to the experimentally-determined binding affinities. In this case, we are concerned about the relative ranking of compounds, not the absolute binding affinity value. 

## DEFINITIONS AND COMMENTS
#### State anything you think is important for someone else to replicate your results. 

## LIMITATIONS
#### What are the limitations of the benchmark? Consider dataset, quality measures, protocol etc. 
#### How could the benchmark be improved?
#### What goals should be hit to make this a "good" benchmark?

The run-to-run variablility is rather high, which might indicate that the benchmark could be improved by running several output structures for each input, rather than just the current one.

This benchmark currently utilizes the ligand scorefunction (an off-shoot of the score12 environment), as this is currently the best protocol for ligand docking in Rosetta. Could be updated for talaris2014, but performance has been shown to deteriorate in the newer scoring environments, notably ref2015 and later. 

This protocol assumes that similar compounds bind in similar modes. 
